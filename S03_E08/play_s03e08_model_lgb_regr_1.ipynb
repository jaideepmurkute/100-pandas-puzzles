{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaideepmurkute/100-pandas-puzzles/blob/master/S03_E08/play_s03e08_model_lgb_regr_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Ec7wkQrQzTZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "186c5059-3436-4f0d-c3a7-17007d8bbf6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install wandb -U -qqq                                           \n",
        "! pip install sklearn -U -qqq\n",
        "! pip install xgboost==1.6.0 #-U -qqq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzjUcHUpQ7C5",
        "outputId": "9a2021cb-5191-40e3-dd95-8d830af94234"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xgboost==1.6.0 in /usr/local/lib/python3.8/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from xgboost==1.6.0) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from xgboost==1.6.0) (1.7.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost\n",
        "xgboost.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FUednWRAZ6l1",
        "outputId": "1c42e34c-58d9-4618-88a1-b6031841678b"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.6.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import joblib\n",
        "\n",
        "sns.set_style('darkgrid')\n",
        "from sklearn.datasets import fetch_california_housing \n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor, IsolationForest\n",
        "from sklearn.linear_model import LinearRegression, SGDOneClassSVM\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "# from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold, TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import torch\n",
        "\n",
        "from scipy.stats.mstats import winsorize\n",
        "from scipy.stats import mode\n",
        "\n",
        "import wandb\n",
        "\n",
        "# import category_encoders as ce\n",
        "\n"
      ],
      "metadata": {
        "id": "_nZvi96Dzcpj"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def set_seeds(config):\n",
        "  np.random.seed(config[\"random_state\"])\n",
        "  random.seed(config[\"random_state\"])\n",
        "  os.environ[\"PYTHONHASHSEED\"] = str(config[\"random_state\"])\n",
        "  '''\n",
        "  torch.manual_seed(config[\"random_state\"])\n",
        "  if torch.cuda.is_available():\n",
        "      torch.cuda.manual_seed(config[\"random_state\"])\n",
        "      torch.cuda.manual_seed_all(config[\"random_state\"])\n",
        "      torch.backends.cudnn.deterministic = True\n",
        "      torch.backends.cudnn.benchmark = True \n",
        "  '''\n",
        "\n",
        "\n",
        "def generate_fold_idx(config, train_df, group_col=None):\n",
        "  if config['fold_split_type'] == 'kfold':\n",
        "    splitter = KFold(n_splits=config['num_folds'], shuffle=True, \n",
        "                                     random_state=config['random_state'])\n",
        "  elif config['fold_split_type'] == 'strat_kfold':\n",
        "    splitter = StratifiedKFold(n_splits=config['num_folds'], shuffle=True, \n",
        "                                     random_state=config['random_state'])\n",
        "  elif config['fold_split_type'] == 'group_kfold':\n",
        "    splitter = GroupKFold(n_splits=config['num_folds'])\n",
        "  elif config['fold_split_type'] == 'time_series_split':  # can use purged as well.\n",
        "    splitter = TimeSeriesSplit(n_splits=config['num_folds'])\n",
        "  else:\n",
        "    raise ValueError(\"fold_split_type {} not recognized... Choose from: \\\n",
        "                    time_series_split, group_time_series_split, purged_time_series_split, kfold\")\n",
        "  \n",
        "  fold_idx_dict = dict()\n",
        "  if config['fold_split_type'] == 'group_kfold':\n",
        "    if group_col in train_df.columns:\n",
        "      for fold_idx, (train_idx, val_idx) in enumerate(splitter.split(X=train_df, \n",
        "                                                                    groups=train_df[group_col].values)):\n",
        "        fold_idx_dict[fold_idx] = dict()\n",
        "        fold_idx_dict[fold_idx]['train_idx'] = train_idx\n",
        "        fold_idx_dict[fold_idx]['val_idx'] = val_idx\n",
        "  else:\n",
        "    # if config['restrict_val_set_to_comp_data']:\n",
        "    #   comp_data_df = train_df[train_df.original_data==False]\n",
        "    #   for fold_idx, (train_idx, val_idx) in enumerate(splitter.split(X=comp_data_df, y=comp_data_df.quality.values)):\n",
        "    #     fold_idx_dict[fold_idx] = dict()\n",
        "    #     fold_idx_dict[fold_idx]['train_idx'] = train_idx\n",
        "    #     fold_idx_dict[fold_idx]['val_idx'] = val_idx\n",
        "    # else:\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(splitter.split(X=train_df, y=train_df.price.values)):\n",
        "      fold_idx_dict[fold_idx] = dict()\n",
        "      fold_idx_dict[fold_idx]['train_idx'] = train_idx\n",
        "      fold_idx_dict[fold_idx]['val_idx'] = val_idx\n",
        "\n",
        "  return fold_idx_dict\n",
        "\n",
        "\n",
        "def save_model(config, model):\n",
        "  # if config['enable_categorical']:\n",
        "  #   model_save_fname = config['model_name'] + '_fold_' + str(config['curr_fold']) + '.json'\n",
        "  # else:\n",
        "  model_save_fname = config['model_name'] + '_fold_' + str(config['curr_fold']) + '.pkl'\n",
        "\n",
        "  \n",
        "  model_local_save_path = config['local_model_dir'] + '/' + model_save_fname\n",
        "  model_drive_save_path = config['drive_model_dir'] + '/' + model_save_fname\n",
        "\n",
        "  print('Saving model...')\n",
        "  # model.save_model(model_local_save_path)\n",
        "  joblib.dump(model, model_local_save_path)\n",
        "\n",
        "  # with open(model_local_save_path, 'wb') as fp:\n",
        "  #   json.dump(model, fp)\n",
        "\n",
        "  print('Copying model to drive...')\n",
        "  shutil.copy(model_local_save_path, model_drive_save_path)\n",
        "  \n",
        "\n",
        "def load_model(config):\n",
        "  # if config['enable_categorical']:\n",
        "  #   model_save_fname = config['model_name'] + '_fold_' + str(config['curr_fold']) + '.json'\n",
        "  # else:\n",
        "  model_save_fname = config['model_name'] + '_fold_' + str(config['curr_fold']) + '.pkl'\n",
        "  \n",
        "  model_local_save_path = config['local_model_dir'] + '/' + model_save_fname\n",
        "  model_drive_save_path = config['drive_model_dir'] + '/' + model_save_fname\n",
        "\n",
        "  if not os.path.exists(model_local_save_path):\n",
        "    shutil.copy(model_drive_save_path, model_local_save_path)\n",
        "  \n",
        "  print('Loading model...')\n",
        "  # model = lgb.LGBMRegressor()\n",
        "  # model.load_model(model_local_save_path)\n",
        "  model = joblib.load(model_local_save_path)\n",
        "\n",
        "  # with open(model_local_save_path, 'rb') as fp:\n",
        "  #   model = json.load(model_local_save_path)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "def get_lgb_params(config):\n",
        "  xgb_params = {\n",
        "            'random_state': config['random_state'], \n",
        "            'n_jobs': config['n_jobs'], \n",
        "            'verbosity': config['verbosity'], \n",
        "            \n",
        "            'boosting_type': config['boosting_type'], \n",
        "            'max_depth': config['max_depth'], \n",
        "            'max_leaves': config['max_leaves'], \n",
        "            'n_estimators': config['n_estimators'], \n",
        "            'early_stopping_rounds': config['early_stopping_rounds'], \n",
        "            \n",
        "            'colsample_bytree': config['colsample_bytree'], \n",
        "            'subsample': config['subsample'], \n",
        "            \n",
        "            # 'enable_categorical': config['enable_categorical'], \n",
        "            \n",
        "            'reg_alpha': config['reg_alpha'], \n",
        "            'reg_lambda': config['reg_lambda'], \n",
        "            'drop_rate': config['drop_rate'], \n",
        "            'max_drop': config['max_drop'], \n",
        "\n",
        "            'max_bin': config['max_bin'], \n",
        "            'min_data_in_leaf': config['min_data_in_leaf'], \n",
        "            \n",
        "            'learning_rate': config['learning_rate'], \n",
        "            'objective': config['objective'], \n",
        "            \n",
        "            'metric': config['eval_metric'],\n",
        "            # 'eval_metric': cohen,\n",
        "\n",
        "        }\n",
        "  \n",
        "  return xgb_params\n",
        "\n"
      ],
      "metadata": {
        "id": "IpZBnKwWzcm7"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_data(config):\n",
        "  for fname in ['train.csv', 'test.csv', 'sample_submission.csv']:\n",
        "    shutil.copy(os.path.join(config['drive_data_dir'], fname), \n",
        "                os.path.join(config['local_data_dir'], fname)\n",
        "                )\n",
        "  train_df = pd.read_csv(os.path.join(config['local_data_dir'], 'train.csv'))\n",
        "  test_df = pd.read_csv(os.path.join(config['local_data_dir'], 'test.csv'))\n",
        "  sub_df = pd.read_csv(os.path.join(config['local_data_dir'], 'sample_submission.csv'))\n",
        "  print(\"Read shape: train_df.shape: \", train_df.shape)\n",
        "  print(\"Read shape: test_df.shape: \", test_df.shape)\n",
        "  print(\"Read shape: sub_df.shape: \", sub_df.shape)\n",
        "  \n",
        "  # train_df.drop(['Id'], axis=1, inplace=True)\n",
        "  train_df['original_data'] = False\n",
        "    \n",
        "  for fname in ['orig_data.csv', 'descriptor_dict.xlsx']:\n",
        "    shutil.copy(os.path.join(config['drive_data_dir'], fname), \n",
        "                os.path.join(config['local_data_dir'], fname)\n",
        "                )\n",
        "  orig_data_df = pd.read_csv(os.path.join(config['local_data_dir'], 'orig_data.csv'))\n",
        "  \n",
        "  # orig_data_df.drop(['#'], axis=1, inplace=True)\n",
        "  orig_data_df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
        "  \n",
        "  orig_data_df['original_data'] = True\n",
        "    \n",
        "  return train_df, test_df, sub_df, orig_data_df\n",
        "  \n",
        "\n",
        "\n",
        "def encode_features(config, train_df, test_df):\n",
        "  cut_encoding_dict = {'Fair': 1, 'Good': 2, 'Very Good': 3, 'Premium': 4, 'Ideal': 5}\n",
        "  train_df['cut'] = train_df['cut'].map(cut_encoding_dict) \n",
        "  test_df['cut'] = test_df['cut'].map(cut_encoding_dict) \n",
        "\n",
        "  color_encoding_dict = {'J': 1, 'I': 2, 'H': 3, 'G': 4, 'F': 5, 'E': 6, 'D': 7}\n",
        "  train_df['color'] = train_df['color'].map(color_encoding_dict) \n",
        "  test_df['color'] = test_df['color'].map(color_encoding_dict) \n",
        "\n",
        "  clarity_encoding_dict = {'FL': 1, 'IF': 2, 'VVS1': 3, 'VVS2': 4, 'VS1': 5, 'VS2': 6, \n",
        "                          'SI1': 7, 'SI2': 8, 'I1': 9, 'I2': 10, 'I3': 11}\n",
        "  train_df['clarity'] = train_df['clarity'].map(clarity_encoding_dict) \n",
        "  test_df['clarity'] = test_df['clarity'].map(clarity_encoding_dict) \n",
        "  \n",
        "  train_df['price_log'] = np.log(train_df['price'].values)\n",
        "  # test_df['price_log'] = np.log(test_df['price'].values)\n",
        "\n",
        "  return train_df, test_df\n",
        "\n",
        "\n",
        "def update_feature_dtypes(config, train_df, test_df):\n",
        "  for col in config['bool_cols']:\n",
        "      train_df[col] = train_df[col].astype('category')\n",
        "      test_df[col] = test_df[col].astype('category')\n",
        "  \n",
        "  return train_df, test_df\n",
        "\n",
        "\n",
        "\n",
        "def get_feature_cols(config, train_df):\n",
        "  config['id_cols'] = ['id', 'original_data']\n",
        "\n",
        "  if config['predict_log']:\n",
        "    config['target_cols'] = ['price_log']\n",
        "  else:\n",
        "    config['target_cols'] = ['price']\n",
        "\n",
        "  all_price_cols = [col for col in train_df.columns if col.startswith('price')]\n",
        "\n",
        "  config['bool_cols'] = []\n",
        "\n",
        "  non_feature_cols = config['target_cols'] + all_price_cols + config['id_cols']\n",
        "\n",
        "  config['feature_cols'] = []\n",
        "  for col in train_df.columns:\n",
        "    if col not in non_feature_cols:\n",
        "      config['feature_cols'].append(col)\n",
        "  \n",
        "  return config\n",
        "\n"
      ],
      "metadata": {
        "id": "eVyxTmmV1mTT"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def scale_data_fn(config, train_df, test_df):\n",
        "  \n",
        "  cols_to_scale = config['feature_cols'] # MAKE SURE THESE ARE ALL CONT. FEATURES.\n",
        "\n",
        "  if config['scaler_type'] == 'standard':\n",
        "    scaler = StandardScaler()\n",
        "  elif config['scaler_type'] == 'robust':\n",
        "    scaler = RobustScaler()\n",
        "  \n",
        "  scaler.fit(train_df[cols_to_scale].values)\n",
        "  train_df[cols_to_scale] = scaler.transform(train_df[cols_to_scale])\n",
        "  test_df[cols_to_scale] = scaler.transform(test_df[cols_to_scale])\n",
        "\n",
        "  return train_df, test_df\n",
        "    "
      ],
      "metadata": {
        "id": "-PDWoJqBpN6x"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def IQR_outlier_handling(train_df, test_df, cols, handling_type):\n",
        "  for col in cols:\n",
        "    # calculate interquartile range\n",
        "    q25, q75 = np.percentile(train_df[col].values, 25), np.percentile(train_df[col].values, 75)\n",
        "    iqr = q75 - q25\n",
        "    \n",
        "    # calculate the outlier cutoff\n",
        "    cut_off = iqr * 1.5\n",
        "    lower_cutoff, upper_cutoff = q25 - cut_off, q75 + cut_off\n",
        "    \n",
        "    num_outliers = train_df[col].loc[(train_df[col] < lower_cutoff) | (train_df[col] > upper_cutoff)].shape[0]\n",
        "    print(\"col: {} \\t # num_outliers: {}\".format(col, num_outliers))\n",
        "\n",
        "    if handling_type == 'remove_train_clip_test':\n",
        "      train_df[col] = train_df[col].loc[(not(train_df[col] < lower_cutoff)) & (not(train_df[col] > upper_cutoff))]\n",
        "      if col in test_df.columns:\n",
        "        test_df[col].loc[test_df[col] < lower_cutoff] = lower_cutoff\n",
        "        test_df[col].loc[test_df[col] > upper_cutoff] = upper_cutoff\n",
        "    elif handling_type == 'clip':\n",
        "      train_df[col].loc[train_df[col] < lower_cutoff] = lower_cutoff\n",
        "      train_df[col].loc[train_df[col] > upper_cutoff] = upper_cutoff\n",
        "      if col in test_df.columns:\n",
        "        test_df[col].loc[test_df[col] < lower_cutoff] = lower_cutoff\n",
        "        test_df[col].loc[test_df[col] > upper_cutoff] = upper_cutoff\n",
        "      \n",
        "    return train_df, test_df\n",
        "\n",
        "\n",
        "\n",
        "def winsorize_outlier_handling(train_df, test_df, cols, lower_lim=0.01, upper_lim=0.98):\n",
        "  # lower_lim = train_df.quantile(0.01)\n",
        "  # upper_lim = train_df.quantile(0.99)\n",
        "  for col in cols:\n",
        "    train_df[col] = winsorize(train_df[col], (lower_lim, upper_lim))\n",
        "    test_df[col] = winsorize(test_df[col], (lower_lim, upper_lim))\n",
        "  \n",
        "  return train_df, test_df\n",
        "\n",
        "\n",
        "\n",
        "def isolation_forest_outlier_handling(train_df, test_df, cols, outlier_thresh=-0.1, \n",
        "                                      handling_method='drop_median', seed=0):\n",
        "  print(\"Training Isolation forest model to detect outliers...\")\n",
        "  iso_forest_model = IsolationForest(n_estimators=500, contamination='auto', random_state=seed)\n",
        "  iso_forest_model.fit(train_df[cols], train_df.MedHouseVal.values)\n",
        "  \n",
        "  sample_scores_train = iso_forest_model.decision_function(train_df[cols])\n",
        "  sample_scores_test = iso_forest_model.decision_function(test_df[cols])\n",
        "\n",
        "  print(\"# train outliers: \", np.sum(sample_scores_train < outlier_thresh))\n",
        "  print(\"# test outliers: \", np.sum(sample_scores_test < outlier_thresh))\n",
        "  \n",
        "  if handling_method == 'drop_median':\n",
        "    print(\"Dropping outlier train samples...\")\n",
        "    # drop train samples and replace test sample values with median from train columns\n",
        "    train_df = train_df.loc[sample_scores_train >= outlier_thresh]\n",
        "    \n",
        "    print(\"Clipping outlier test samples to median value...\")\n",
        "    for col in cols:\n",
        "      test_df[col].loc[sample_scores_test < outlier_thresh] = train_df[col].median(axis=0)\n",
        "  elif handling_method == 'winsorize':\n",
        "    train_df.loc[sample_scores_train < outlier_thresh] = winsorize_outlier_handling(train_df.loc[sample_scores_train < outlier_thresh], \n",
        "                                                                  cols, lower_lim=0.01, upper_lim=0.98)\n",
        "    test_df.loc[sample_scores_test < outlier_thresh] = winsorize_outlier_handling(test_df.loc[sample_scores_test < outlier_thresh], \n",
        "                                                                  cols, lower_lim=0.01, upper_lim=0.98)\n",
        "    \n",
        "  return train_df, test_df\n",
        "\n",
        "\n",
        "def manual_outlier_handling(config, train_df, test_df):\n",
        "  train_df[train_df.made==10000].made = 2000  # Not sure what other year to set here. \n",
        "\n",
        "  return train_df, test_df \n",
        "\n",
        "\n",
        "def handle_outliers(config, train_df, test_df, cols):\n",
        "  if config['outlier_handling_method'] == 'manual':\n",
        "    train_df, test_df = manual_outlier_handling(train_df, test_df, cols=cols)\n",
        "  if config['outlier_handling_method'] == 'winsorize':\n",
        "    train_df, test_df = winsorize_outlier_handling(train_df, test_df, cols=cols)\n",
        "  elif config['outlier_handling_method'] == 'iso_forest':\n",
        "    train_df, test_df = isolation_forest_outlier_handling(train_df, test_df, cols=cols, \n",
        "                                  outlier_thresh=-0.1, handling_method='drop_median', seed=config['seed'])\n",
        "  elif config['outlier_handling_method'] == 'iqr':\n",
        "    train_df, test_df = IQR_outlier_handling(train_df, test_df, cols, handling_type='clip')\n",
        "\n",
        "  return train_df, test_df\n"
      ],
      "metadata": {
        "id": "Ul39e-gKyZga"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_config():\n",
        "  return config\n",
        "\n",
        "\n",
        "def save_config(config):\n",
        "  config_to_save = {}  # to avoid types like object or others that somtimes cause problem reading data back.\n",
        "  for k, v in config.items():\n",
        "    if isinstance(v, (bool, int, float, str, list, dict)):\n",
        "      config_to_save[k] = v\n",
        "  \n",
        "  config_local_save_path = os.path.join(config['local_model_dir'], 'saved_config.json')\n",
        "  config_drive_save_path = os.path.join(config['drive_model_dir'], 'saved_config.json')\n",
        "  \n",
        "  with open(config_local_save_path, 'w') as fp:\n",
        "    json.dump(config_to_save, fp, indent=4, sort_keys=True)\n",
        "  \n",
        "  shutil.copy(config_local_save_path, config_drive_save_path)\n",
        "\n",
        "\n",
        "def get_model_config(config):\n",
        "  config_local_save_path = os.path.join(config['local_model_dir'], 'saved_config.json')\n",
        "  config_drive_save_path = os.path.join(config['drive_model_dir'], 'saved_config.json')\n",
        "  \n",
        "  shutil.copy(config_drive_save_path, config_local_save_path)\n",
        "\n",
        "  with open(config_local_save_path, 'r') as fp:\n",
        "    model_config = json.load(fp)\n",
        "  \n",
        "  return model_config\n",
        "  \n",
        "\n",
        "def across_col_feat_v1(config, df):\n",
        "  '''\n",
        "  https://www.kaggle.com/competitions/playground-series-s3e8/discussion/389472\n",
        "  Features suggested by chatGPT.\n",
        "  '''\n",
        "\n",
        "  feat_cols = []\n",
        "  for col in df.columns:\n",
        "    if col not in ['id', 'price', 'original_data']:\n",
        "      feat_cols.append(col)\n",
        "  \n",
        "  df['volume'] = df['x'] * df['y'] * df['z']\n",
        "  df['density'] = df['carat'] / df['volume']\n",
        "  df['table_percentage'] = (df['table'] / ((df['x'] + df['y']) / 2)) * 100\n",
        "  df['depth_percentage'] = (df['depth'] / ((df['x'] + df['y']) / 2)) * 100\n",
        "  df['symmetry'] = (abs(df['x'] - df['z']) + abs(df['y'] - df['z'])) / (df['x'] + df['y'] + df['z'])\n",
        "  df['surface_area'] = 2 * ((df['x'] * df['y']) + (df['x'] * df['z']) + (df['y'] * df['z']))\n",
        "  df['depth_to_table_ratio'] = df['depth'] / df['table']\n",
        "  \n",
        "  return df\n",
        "\n",
        "\n",
        "def extract_features(config, train_df, test_df):\n",
        "  '''\n",
        "  Keep is min: original data set is imaginary.\n",
        "  We can best aim for finding logic dataset author may have used to put in price values.\n",
        "  '''\n",
        "  if config['feature_version'] is None:\n",
        "    pass\n",
        "  elif config['feature_version'] == 'v1':\n",
        "    train_df = across_col_feat_v1(config, train_df)\n",
        "    test_df = across_col_feat_v1(config, test_df)\n",
        "  else:\n",
        "    print(\"Feature version {} not supported. Choose from : None, v1\")\n",
        "  \n",
        "  return train_df, test_df\n",
        "    \n",
        "\n",
        "# consider prior distribution of labels?\n",
        "def find_optimal_cuts():\n",
        "  pass\n",
        "\n",
        "\n",
        "def preprocess(config, train_df, test_df):\n",
        "    val = 1e-2\n",
        "    train_df['x'] = train_df['x'].replace(0, val)\n",
        "    train_df['y'] = train_df['z'].replace(0, val)\n",
        "    train_df['z'] = train_df['z'].replace(0, val)\n",
        "\n",
        "    train_df['depth'].fillna(train_df['depth'].median(), inplace=True)\n",
        "\n",
        "    # fill nans\n",
        "    # fill infs\n",
        "\n",
        "    raise_exp = False\n",
        "    for col in train_df.columns:\n",
        "      if col in ['id']:\n",
        "        continue\n",
        "      nan_cnt = np.sum(np.isnan(train_df[col].values))\n",
        "      inf_cnt = np.sum(np.isinf(train_df[col].values))\n",
        "      if nan_cnt > 0:\n",
        "        print(\"col: {} has {} nans...\".format(col, nan_cnt))\n",
        "        raise_exp = True\n",
        "      if inf_cnt > 0:\n",
        "        print(\"col: {} has {} infs...\".format(col, inf_cnt))\n",
        "        raise_exp = True\n",
        "    \n",
        "    \n",
        "    if raise_exp:\n",
        "      print(\"Nans/Infs detected...\")\n",
        "      raise\n",
        "    \n",
        "    return train_df, test_df\n"
      ],
      "metadata": {
        "id": "dLHcvaiBLFJs"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_k_folds():\n",
        "\n",
        "  # needed becuse variable that is updated within function becomes a local variable and has to be passed in.\n",
        "  config = get_config()\n",
        "  \n",
        "  create_paths(config)\n",
        "  train_df, test_df, sub_df, orig_data_df = get_data(config)\n",
        "  print(\"train_df.shape: \", train_df.shape)\n",
        "  print(\"test_df.shape: \", test_df.shape)\n",
        "  print(\"sub_df.shape: \", sub_df.shape)\n",
        "  print(\"orig_data_df.shape: \", orig_data_df.shape)\n",
        "  \n",
        "  # ---------------\n",
        "\n",
        "  if config['include_orig_data']:\n",
        "    if config['validate_only_comp_data']:\n",
        "      fold_idx_dict = generate_fold_idx(config, train_df)\n",
        "      train_df = pd.concat((train_df, orig_data_df), axis=0)\n",
        "      print(\"After appending orig data to train data: \")\n",
        "      print(\"train_df.shape: \", train_df.shape)\n",
        "    else:\n",
        "      train_df = pd.concat((train_df, orig_data_df), axis=0)\n",
        "      print(\"After appending orig data to train data: \")\n",
        "      print(\"train_df.shape: \", train_df.shape)\n",
        "      fold_idx_dict = generate_fold_idx(config, train_df)\n",
        "  else:\n",
        "    fold_idx_dict = generate_fold_idx(config, train_df)\n",
        "  \n",
        "  for fold_num in fold_idx_dict.keys():\n",
        "    val_idx = fold_idx_dict[fold_num]['val_idx']\n",
        "    all_idx = np.arange(0, train_df.shape[0])\n",
        "    train_idx = np.setdiff1d(all_idx, val_idx)\n",
        "    fold_idx_dict[fold_num]['train_idx'] = train_idx\n",
        "  \n",
        "  print('id in train_df.columns: ', id in train_df.columns)\n",
        "  # ---------------\n",
        "\n",
        "  config = get_feature_cols(config, train_df)\n",
        "  train_df, test_df = encode_features(config, train_df, test_df)\n",
        "  print(\"After feature encoding: train_df.shape: \", train_df.shape)\n",
        "  print(\"After feature encoding: test_df.shape: \", test_df.shape)\n",
        "  \n",
        "  # ---------------\n",
        "  \n",
        "  train_df, test_df = preprocess(config, train_df, test_df)\n",
        "  print(\"After preprocessing: train_df.shape: \", train_df.shape)\n",
        "  print(\"After preprocessing: test_df.shape: \", test_df.shape)\n",
        "\n",
        "  # ---------------\n",
        "  \n",
        "  train_df, test_df = extract_features(config, train_df, test_df)\n",
        "  print(\"After feature extraction: train_df.shape: \", train_df.shape)\n",
        "  print(\"After feature extraction: test_df.shape: \", test_df.shape)\n",
        "  \n",
        "  # ---------------\n",
        "  \n",
        "  if config['enable_categorical']:\n",
        "    train_df, test_df = update_feature_dtypes(config, train_df, test_df)\n",
        "  \n",
        "  # ---------------\n",
        "  \n",
        "  config = get_feature_cols(config, train_df)\n",
        "  print(\"config['feature_cols']): \", config['feature_cols'])\n",
        "  print(\"# feature_cols: \", len(config['feature_cols']))\n",
        "\n",
        "  # ---------------\n",
        "  \n",
        "  if config['handle_outliers']:\n",
        "    print(\"Before outlier handling: \")\n",
        "    print(f\"train_df.shape: {train_df.shape} \\t test_df.shape: {test_df.shape}\")\n",
        "    train_df, test_df = handle_outliers(config, train_df, test_df)\n",
        "    print(\"After outlier handling: \")\n",
        "    print(f\"train_df.shape: {train_df.shape} \\t test_df.shape: {test_df.shape}\")\n",
        "  \n",
        "  # ---------------\n",
        "  \n",
        "  if config['scale_data']:\n",
        "    train_df, test_df = scale_data_fn(config, train_df, test_df)\n",
        "  \n",
        "  # ------------------\n",
        "  \n",
        "  # fold_idx_dict = generate_fold_idx(config, train_df)\n",
        "  \n",
        "  per_model_metrics = {\n",
        "                        'loss': {'train': [], 'val': []},\n",
        "                        'rmse': {'train': [], 'val': []},\n",
        "                      }\n",
        "  \n",
        "  shutil.copy('/content/drive/MyDrive/Playground Series/S03_E08/code/play_s03e08_model_lgb_regr_1.ipynb', \n",
        "              os.path.join(config['drive_model_dir'], 'play_s03e08_model_lgb_regr_1.ipynb'))\n",
        "  save_config(config)\n",
        "\n",
        "  # fold_model_name = config['model_name'] + '_fold_' + str(config['curr_fold'])\n",
        "  # if config['use_wandb']:\n",
        "  wandb.init(name=config['model_name'], project=config['project_name'], \n",
        "            tags=['baseline'], config=config)\n",
        "  if config['choice'] == 3:\n",
        "    print(\"Updating sweep configs...\")\n",
        "    for k, v in wandb.config.items():\n",
        "      config[k] = v\n",
        "    print(\"*** Updated sweep config: \", config)\n",
        "    \n",
        "  for fold_num in range(config['num_folds']):\n",
        "    if fold_num not in config['folds_to_train']:\n",
        "      continue\n",
        "\n",
        "    print(\"Training fold: \", fold_num)\n",
        "    config['curr_fold'] = fold_num\n",
        "\n",
        "    # -----------\n",
        "    # fold_model_name = config['model_name'] + '_fold_' + str(config['curr_fold'])\n",
        "    # # if config['use_wandb']:\n",
        "    # wandb.init(name=fold_model_name, project=config['project_name'], \n",
        "    #           tags=['baseline'], config=config)\n",
        "    # if config['choice'] == 3:\n",
        "    #   print(\"Updating sweep configs...\")\n",
        "    #   for k, v in wandb.config.items():\n",
        "    #     config[k] = v\n",
        "    #   print(\"*** Updated sweep config: \", config)\n",
        "    \n",
        "    set_seeds(config)\n",
        "    \n",
        "    # -----------\n",
        "    \n",
        "    train_idx = fold_idx_dict[fold_num]['train_idx']\n",
        "    val_idx = fold_idx_dict[fold_num]['val_idx']\n",
        "    print(\"len(train_idx): {} \\t len(val_idx): {}\".format(len(train_idx), len(val_idx)))\n",
        "    \n",
        "    train_data = train_df[config['feature_cols']].iloc[train_idx]\n",
        "    train_label = train_df[config['target_cols']].iloc[train_idx] \n",
        "    if config['predict_log']:\n",
        "      train_label_price = train_df['price'].iloc[train_idx]  # since train label can be price or price_log\n",
        "\n",
        "    val_data = train_df[config['feature_cols']].iloc[val_idx]\n",
        "    val_label = train_df[config['target_cols']].iloc[val_idx]\n",
        "    if config['predict_log']:\n",
        "      val_label_price = train_df['price'].iloc[val_idx]\n",
        "\n",
        "    lgb_params = get_lgb_params(config)\n",
        "\n",
        "    # if config['scale_pos_weight'] == 'auto':\n",
        "    #   auto_pos_cls_weight = train_label[train_label.values==0].shape[0] / train_label[train_label.values==1].shape[0]\n",
        "    #   print(\"Setting scale_pos_weight to: \", auto_pos_cls_weight)\n",
        "    #   xgb_params['scale_pos_weight'] = auto_pos_cls_weight\n",
        "    # else:\n",
        "    #   xgb_params['scale_pos_weight'] = config['scale_pos_weight']\n",
        "    \n",
        "    # if config['eval_metric'] == 'wqkappa':\n",
        "    #   xgb_params['disable_default_eval_metric'] = 1\n",
        "    #   xgb_params['eval_metric'] = wqkappa\n",
        "\n",
        "    model = lgb.LGBMRegressor(**lgb_params)\n",
        "\n",
        "    print(\"Training model ...\")\n",
        "    model.fit(train_data, train_label, \n",
        "              eval_set=[(train_data, train_label), \n",
        "                        (val_data, val_label)], \n",
        "              verbose=50, \n",
        "              )\n",
        "    \n",
        "    print(\"Saving Model...\")\n",
        "    save_model(config, model)\n",
        "\n",
        "    # -----------\n",
        "    # print(model.evals_result())\n",
        "    # if config['choice'] == 1:\n",
        "    #   for i in range(len(model.evals_result()['validation_0'][config['eval_metric']])):\n",
        "    #     wandb.log({\n",
        "    #       f\"Per Epoch Train {config['eval_metric']}\": model.evals_result()['validation_0'][config['eval_metric']][i], \n",
        "    #       f\"Per Epoch Val {config['eval_metric']}\": model.evals_result()['validation_1'][config['eval_metric']][i], \n",
        "    #       }\n",
        "    #     )\n",
        "      \n",
        "\n",
        "    # print(\"Best val epoch: max validation_1 metric: \", \n",
        "    #       np.min(model.evals_result()['validation_1'][config['eval_metric']]))\n",
        "    # print(\"Best val epoch number:  \", np.argmin(model.evals_result()['validation_1'][config['eval_metric']]))\n",
        "    # print(\"model.best_ntree_limit: \", model.best_ntree_limit)\n",
        "    print(\"model.best_iteration_: \", model.best_iteration_)\n",
        "    print(\"model.best_score_: \", model.best_score_)\n",
        "    print(\"best rmse score: \", model.best_score_['valid_1']['rmse'])\n",
        "    max_iter = np.argmin(model.evals_result_['valid_1']['rmse'])\n",
        "    \n",
        "    # ------------------\n",
        "\n",
        "    train_preds = model.predict(train_data, num_iteration=max_iter)\n",
        "    val_preds = model.predict(val_data, num_iteration=max_iter)\n",
        "    if config['predict_log']:\n",
        "      train_preds_price = np.exp(train_preds)\n",
        "      val_preds_price = np.exp(val_preds)\n",
        "    \n",
        "    # ------------------\n",
        "\n",
        "    \n",
        "    if config['predict_log']:\n",
        "      train_mse = mean_squared_error(train_label_price.values, train_preds_price)\n",
        "      val_mse = mean_squared_error(val_label_price.values, val_preds_price)\n",
        "      \n",
        "      train_rmse = mean_squared_error(train_label_price.values, train_preds_price, squared=False)\n",
        "      val_rmse = mean_squared_error(val_label_price.values, val_preds_price, squared=False)\n",
        "    else:\n",
        "      train_mse = mean_squared_error(train_label.values, train_preds)\n",
        "      val_mse = mean_squared_error(val_label.values, val_preds)\n",
        "    \n",
        "      train_rmse = mean_squared_error(train_label.values, train_preds, squared=False)\n",
        "      val_rmse = mean_squared_error(val_label.values, val_preds, squared=False)\n",
        "      \n",
        "    # ------------------\n",
        "    \n",
        "\n",
        "\n",
        "    per_model_metrics['loss']['train'].append(train_mse)\n",
        "    per_model_metrics['loss']['val'].append(val_mse)\n",
        "    per_model_metrics['rmse']['train'].append(train_rmse)\n",
        "    per_model_metrics['rmse']['val'].append(val_rmse)\n",
        "    \n",
        "    print(f\"MSE Loss: Train: {train_mse} \\t Val: {val_mse}\")\n",
        "    print(f\"RMSE: Train: {train_rmse} \\t Val: {val_rmse}\")\n",
        "    \n",
        "    # wandb.log({\n",
        "    #     \"Best Epoch Train AUROC\": train_auroc, \n",
        "    #     \"Best Epoch Val AUROC\": val_auroc,\n",
        "\n",
        "    #     \"Best Epoch Train F1\": train_f1, \n",
        "    #     \"Best Epoch Val F1\": val_f1,\n",
        "        \n",
        "    #     \"Best Epoch Train Log Loss\": train_logloss, \n",
        "    #     \"Best Epoch Val Log Loss\": val_logloss,\n",
        "    #     }\n",
        "    #   )\n",
        "\n",
        "    if config['choice'] == 1:\n",
        "      plt.figure(figsize=(12, 10))\n",
        "      lgb.plot_importance(model)\n",
        "      plt.title(f\"Fold: {fold_num}\")\n",
        "      plt.show()\n",
        "      plt.close()\n",
        "\n",
        "    print('-'*30)\n",
        "  \n",
        "  print(\"Fold average stats.: \")\n",
        "  print(f\"RMSE: Train: {np.mean(per_model_metrics['rmse']['train'])} \\t Val: {np.mean(per_model_metrics['rmse']['val'])}\")\n",
        "  print(f\"Loss: Train: {np.mean(per_model_metrics['loss']['train'])} \\t Val: {np.mean(per_model_metrics['loss']['val'])}\")\n",
        "  print(\"per fold VAL RMSEs: \", per_model_metrics['rmse']['val'])\n",
        "\n",
        "  wandb.log({\n",
        "        \"Fold average train rmse\": np.mean(per_model_metrics['rmse']['train']), \n",
        "        \"Fold average val rmse\": np.mean(per_model_metrics['rmse']['val']),\n",
        "        }\n",
        "      )\n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZEEIFAjRzckV"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_paths(config):\n",
        "  config['local_model_dir'] = '/content/model_store'\n",
        "  config['drive_model_dir'] = os.path.join(config['drive_project_dir'], 'model_store')\n",
        "  \n",
        "  if not os.path.exists(config['local_model_dir']):\n",
        "    os.mkdir(config['local_model_dir'])\n",
        "  \n",
        "  if not os.path.exists(config['drive_model_dir']):\n",
        "    os.mkdir(config['drive_model_dir'])\n",
        "  \n",
        "  # -------------\n",
        "\n",
        "  config['local_model_dir'] = os.path.join(config['local_model_dir'], config['model_name']) \n",
        "  config['drive_model_dir'] = os.path.join(config['drive_model_dir'], config['model_name']) \n",
        "\n",
        "  if not os.path.exists(config['local_model_dir']): \n",
        "    os.mkdir(config['local_model_dir'])\n",
        "  if not os.path.exists(config['drive_model_dir']): \n",
        "    os.mkdir(config['drive_model_dir'])\n",
        "  \n",
        "  # -------------\n",
        "\n",
        "  config['local_data_dir'] = '/content/data'\n",
        "  config['drive_data_dir'] = os.path.join(config['drive_project_dir'], 'data/')\n",
        "\n",
        "  config['local_feature_dir'] = '/content/feature_store'\n",
        "  config['drive_feature_dir'] = os.path.join(config['drive_project_dir'], 'feature_store')\n",
        "\n",
        "  if not os.path.exists(config['local_data_dir']):\n",
        "    os.mkdir(config['local_data_dir'])\n",
        "  \n",
        "  if not os.path.exists(config['local_feature_dir']):\n",
        "    os.mkdir(config['local_feature_dir'])\n",
        "\n"
      ],
      "metadata": {
        "id": "cAhSGQuxzchu"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def aggregate_preds(config, per_fold_test_preds, per_model_metrics=None): \n",
        "  \n",
        "  if config['aggr_type'] == 'simple':\n",
        "    aggr_test_preds_cont = np.mean(per_fold_test_preds, axis=1).flatten()\n",
        "  elif config['aggr_type'] == 'rmse_weighted':\n",
        "    aggr_test_preds_cont = np.average(per_fold_test_preds, axis=1, \n",
        "                                      weights=1/np.array(per_model_metrics['rmse']['val'])).flatten()\n",
        "  elif config['aggr_type'] == 'loss_weighted':\n",
        "    aggr_test_preds_cont = np.average(per_fold_test_preds, axis=1, \n",
        "                                      weights=1/np.array(per_model_metrics['loss']['val'])).flatten()\n",
        "  else:\n",
        "    print(\"aggr_type: {} not supported...\".format(config['aggr_type']))\n",
        "    raise\n",
        "\n",
        "  return aggr_test_preds_cont\n",
        "\n",
        "\n",
        "\n",
        "def test_model(config):\n",
        "  create_paths(config)\n",
        "\n",
        "  train_df, test_df, sub_df, orig_data_df = get_data(config)\n",
        "  print(\"train_df.shape: \", train_df.shape)\n",
        "  print(\"test_df.shape: \", test_df.shape)\n",
        "  print(\"sub_df.shape: \", sub_df.shape)\n",
        "  print(\"orig_data_df.shape: \", orig_data_df.shape)\n",
        "  test_ids = test_df.id.values\n",
        "  \n",
        "  # ------------------------------\n",
        "  model_training_config = get_model_config(config)\n",
        "  print(\"model_training_config: \", model_training_config)\n",
        "  for key in ['scale_data', 'scaler_type', 'handle_outliers', 'predict_log', \n",
        "              'outlier_handling_method', 'feature_version', 'include_orig_data', \n",
        "              'fold_split_type', 'num_folds', 'random_state', 'validate_only_comp_data', \n",
        "              'enable_categorical']:  \n",
        "    if key in model_training_config.keys():\n",
        "      # print(f\"Overwriting value for {key} with: {model_training_config[key]}\")\n",
        "      config[key] = model_training_config[key]\n",
        "  \n",
        "  \n",
        "  # ---------------\n",
        "  \n",
        "  if config['include_orig_data']:\n",
        "    if config['validate_only_comp_data']:\n",
        "      fold_idx_dict = generate_fold_idx(config, train_df)\n",
        "      train_df = pd.concat((train_df, orig_data_df), axis=0)\n",
        "      print(\"After appending orig data to train data: \")\n",
        "      print(\"train_df.shape: \", train_df.shape)\n",
        "    else:\n",
        "      train_df = pd.concat((train_df, orig_data_df), axis=0)\n",
        "      print(\"After appending orig data to train data: \")\n",
        "      print(\"train_df.shape: \", train_df.shape)\n",
        "      fold_idx_dict = generate_fold_idx(config, train_df)\n",
        "  else:\n",
        "    fold_idx_dict = generate_fold_idx(config, train_df)\n",
        "  \n",
        "  for fold_num in fold_idx_dict.keys():\n",
        "    val_idx = fold_idx_dict[fold_num]['val_idx']\n",
        "    all_idx = np.arange(0, train_df.shape[0])\n",
        "    train_idx = np.setdiff1d(all_idx, val_idx)\n",
        "    fold_idx_dict[fold_num]['train_idx'] = train_idx\n",
        "  \n",
        "  # ------------------------------\n",
        "\n",
        "  config = get_feature_cols(config, train_df)\n",
        "  train_df, test_df = encode_features(config, train_df, test_df)\n",
        "  print(\"After feature encoding: train_df.shape: \", train_df.shape)\n",
        "  print(\"After feature encoding: test_df.shape: \", test_df.shape)\n",
        "  \n",
        "  # ---------------\n",
        "  train_df, test_df = preprocess(config, train_df, test_df)\n",
        "  print(\"After preprocessing 1: train_df.shape: \", train_df.shape)\n",
        "  print(\"After preprocessing 2: test_df.shape: \", test_df.shape)\n",
        "\n",
        "  # ---------------\n",
        "  \n",
        "  print(\"Before feature extraction: train_df.shape: \", train_df.shape)\n",
        "  print(\"Before feature extraction: test_df.shape: \", test_df.shape)\n",
        "  train_df, test_df = extract_features(config, train_df, test_df)\n",
        "  print(\"After feature extraction: train_df.shape: \", train_df.shape)\n",
        "  print(\"After feature extraction: test_df.shape: \", test_df.shape)\n",
        "  \n",
        "  # ---------------\n",
        "  \n",
        "  if config['enable_categorical']:\n",
        "    train_df, test_df = update_feature_dtypes(config, train_df, test_df)\n",
        "  \n",
        "  # ---------------\n",
        "  \n",
        "  config = get_feature_cols(config, train_df)\n",
        "  print(\"# feature_cols: \", len(config['feature_cols']))\n",
        "  \n",
        "  # ------------------\n",
        "  \n",
        "  if config['handle_outliers']:\n",
        "    print(\"Before outlier handling: \")\n",
        "    print(f\"train_df.shape: {train_df.shape} \\t test_df.shape: {test_df.shape}\")\n",
        "    train_df, test_df = handle_outliers(config, train_df, test_df)\n",
        "    print(\"After outlier handling: \")\n",
        "    print(f\"train_df.shape: {train_df.shape} \\t test_df.shape: {test_df.shape}\")\n",
        "  \n",
        "  if config['scale_data']:\n",
        "    print(\"Scaling data...\")\n",
        "    train_df, test_df = scale_data_fn(config, train_df, test_df)\n",
        "    \n",
        "\n",
        "  # ------------------\n",
        "\n",
        "  # fold_idx_dict = generate_fold_idx(config, train_df)\n",
        "  \n",
        "  per_model_metrics = {\n",
        "                        'loss': {'train': [], 'val': []},\n",
        "                       'rmse': {'train': [], 'val': []},\n",
        "                      }\n",
        "\n",
        "  per_fold_test_preds = None\n",
        "  pred_cnt = 0\n",
        "\n",
        "  for fold_num in range(config['num_folds']):\n",
        "    if fold_num not in config['folds_to_train']:\n",
        "      continue\n",
        "    pred_cnt += 1\n",
        "    print(\"Training fold: \", fold_num)\n",
        "    config['curr_fold'] = fold_num\n",
        "\n",
        "    # -----------\n",
        "    fold_model_name = config['model_name'] + '_fold_' + str(config['curr_fold'])\n",
        "\n",
        "    train_idx = fold_idx_dict[fold_num]['train_idx']\n",
        "    val_idx = fold_idx_dict[fold_num]['val_idx']\n",
        "    print(\"len(train_idx): {} \\t len(val_idx): {}\".format(len(train_idx), len(val_idx)))\n",
        "    \n",
        "    train_data = train_df[config['feature_cols']].iloc[train_idx]\n",
        "    train_label = train_df[config['target_cols']].iloc[train_idx]\n",
        "    if config['predict_log']:\n",
        "      train_label_price = train_df['price'].iloc[train_idx]  # since train label can be price or price_log\n",
        "\n",
        "    val_data = train_df[config['feature_cols']].iloc[val_idx]\n",
        "    val_label = train_df[config['target_cols']].iloc[val_idx]\n",
        "    if config['predict_log']:\n",
        "      val_label_price = train_df['price'].iloc[val_idx]\n",
        "\n",
        "    test_data = test_df[config['feature_cols']]\n",
        "    \n",
        "    # --------\n",
        "\n",
        "    model = load_model(config)\n",
        "    \n",
        "    max_iter = np.argmin(model.evals_result_['valid_1']['rmse'])\n",
        "    # max_iter = model.best_iteration_\n",
        "\n",
        "    train_preds = model.predict(train_data, num_iteration=max_iter)  # model.best_iteration_\n",
        "    val_preds = model.predict(val_data, num_iteration=max_iter)  # model.best_iteration_\n",
        "    test_preds = model.predict(test_data, ntree_limit=max_iter)  # model.best_iteration_\n",
        "    \n",
        "    if config['predict_log']:\n",
        "      train_preds_price = np.exp(train_preds)\n",
        "      val_preds_price = np.exp(val_preds)\n",
        "      test_preds = np.exp(test_preds)\n",
        "    \n",
        "    # val_preds = np.clip(val_preds, np.min(train_label.values), np.max(train_label.values))\n",
        "    # test_preds = np.clip(test_preds, np.min(train_label.values), np.max(train_label.values))\n",
        "    \n",
        "    # ------------------\n",
        "\n",
        "    # train_mse = mean_squared_error(train_label.values, train_preds)\n",
        "    # val_mse = mean_squared_error(val_label.values, val_preds)\n",
        "    \n",
        "    # train_rmse = mean_squared_error(train_label.values, train_preds, squared=False)\n",
        "    # val_rmse = mean_squared_error(val_label.values, val_preds, squared=False)\n",
        "    if config['predict_log']:\n",
        "      train_mse = mean_squared_error(train_label_price.values, train_preds_price)\n",
        "      val_mse = mean_squared_error(val_label_price.values, val_preds_price)\n",
        "      \n",
        "      train_rmse = mean_squared_error(train_label_price.values, train_preds_price, squared=False)\n",
        "      val_rmse = mean_squared_error(val_label_price.values, val_preds_price, squared=False)\n",
        "    else:\n",
        "      train_mse = mean_squared_error(train_label.values, train_preds)\n",
        "      val_mse = mean_squared_error(val_label.values, val_preds)\n",
        "    \n",
        "      train_rmse = mean_squared_error(train_label.values, train_preds, squared=False)\n",
        "      val_rmse = mean_squared_error(val_label.values, val_preds, squared=False)\n",
        "      \n",
        "\n",
        "    # ------------------\n",
        "    \n",
        "    per_model_metrics['loss']['train'].append(train_mse)\n",
        "    per_model_metrics['loss']['val'].append(val_mse)\n",
        "    per_model_metrics['rmse']['train'].append(train_rmse)\n",
        "    per_model_metrics['rmse']['val'].append(val_rmse)\n",
        "\n",
        "    print(f\"MSE: Train: {train_mse} \\t Val: {val_mse}\")\n",
        "    print(f\"RMSE: Train: {train_rmse} \\t Val: {val_rmse}\")\n",
        "    \n",
        "    \n",
        "    test_preds = np.reshape(test_preds, newshape=(test_preds.shape[0], 1))\n",
        "    \n",
        "    if per_fold_test_preds is None:\n",
        "      per_fold_test_preds = test_preds\n",
        "    else:\n",
        "      per_fold_test_preds = np.concatenate((per_fold_test_preds, test_preds), axis=1)\n",
        "     \n",
        "\n",
        "  print(\"Fold average stats.: \")\n",
        "  print(f\"RMSE: Train: {np.mean(per_model_metrics['rmse']['train'])} \\t Val: {np.mean(per_model_metrics['rmse']['val'])}\")\n",
        "  print(f\"MSE Loss: Train: {np.mean(per_model_metrics['loss']['train'])} \\t Val: {np.mean(per_model_metrics['loss']['val'])}\")\n",
        "  \n",
        "  # ------------------\n",
        "\n",
        "  print(\"per_fold_test_preds.shape: \", per_fold_test_preds.shape)\n",
        "  \n",
        "  test_preds_aggr = aggregate_preds(config, per_fold_test_preds, per_model_metrics)\n",
        "  print(\"test_preds_aggr[:10]: \", test_preds_aggr[:10])\n",
        "\n",
        "  sub_df = pd.DataFrame([])\n",
        "  sub_df['id'] = test_ids \n",
        "  sub_df['price'] = test_preds_aggr\n",
        "  sub_df.to_csv(os.path.join(config['local_model_dir'], 'sample_submission.csv'), index=False)\n",
        "  shutil.copy(os.path.join(config['local_model_dir'], 'sample_submission.csv'), \n",
        "              os.path.join(config['drive_model_dir'], 'sample_submission.csv'))\n",
        "\n",
        "\n",
        "# submission_{notebook_name}_{date}_{time}.csv\n"
      ],
      "metadata": {
        "id": "p02CccbvRZPI"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def model_analysis():\n",
        "  config = get_config()\n",
        "  create_paths(config)\n",
        "  \n",
        "  train_df, test_df, sub_df = get_data(config)\n",
        "  print(\"train_df.shape: \", train_df.shape)\n",
        "  test_ids = test_df.Id.values\n",
        "\n",
        "  train_df, test_df = encode_features(config, train_df, test_df)\n",
        "\n",
        "  config = get_feature_cols(config, train_df)\n",
        "\n",
        "  fold_idx_dict = generate_fold_idx(config, train_df)\n",
        "  \n",
        "  for fold_num in range(config['num_folds']):\n",
        "    if fold_num not in config['folds_to_train']:\n",
        "      continue\n",
        "    config['curr_fold'] = fold_num\n",
        "\n",
        "    # -----------\n",
        "    # fold_model_name = config['model_name'] + '_fold_' + str(config['curr_fold'])\n",
        "\n",
        "    # train_idx = fold_idx_dict[fold_num]['train_idx']\n",
        "    # val_idx = fold_idx_dict[fold_num]['val_idx']\n",
        "    # print(\"len(train_idx): {} \\t len(val_idx): {}\".format(len(train_idx), len(val_idx)))\n",
        "    \n",
        "    # train_data = train_df[config['feature_cols']].iloc[train_idx]\n",
        "    # train_label = train_df[config['target_cols']].iloc[train_idx]\n",
        "\n",
        "    # val_data = train_df[config['feature_cols']].iloc[val_idx]\n",
        "    # val_label = train_df[config['target_cols']].iloc[val_idx]\n",
        "\n",
        "    # test_data = test_df[config['feature_cols']]\n",
        "    \n",
        "    # --------\n",
        "\n",
        "    model = load_model(config)\n",
        "    plt.figure(figsuze=(12, 10))\n",
        "    xgb.plot_importance(model)\n",
        "    plt.title(f\"Fold: {fold_num}\")\n",
        "    plt.show()\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "8W7JhZgr88if"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "config = {\n",
        "    'choice': 2, \n",
        "    'random_state': 21, \n",
        "    'aggr_type': 'rmse_weighted',  # simple / wqkappa_weighted / loss_weighted\n",
        "    \n",
        "    'model_name': 'lgb_model_1', \n",
        "    \n",
        "    'include_orig_data': True, \n",
        "\n",
        "    'feature_version': 'v1',  # None, v1, v2, v3\n",
        "    \n",
        "    'predict_log': False, # to train model to predict log(price); for price is a heavy tailed distribution\n",
        " \n",
        "    'handle_outliers': False, \n",
        "    'outlier_handling_method': 'winsorize', \n",
        "    \n",
        "    'scale_data': False, \n",
        "    'scaler_type': 'standard',  # standard / robust\n",
        "\n",
        "    'enable_categorical': False, \n",
        "\n",
        "    'fold_split_type': 'kfold',  # kfold, strat_kfold\n",
        "    'num_folds': 10, \n",
        "    'folds_to_train': [0,1,2,3,4,5,6,7,8,9], #5,6,7,8,9], #,5,6,7,8,9,10], \n",
        "    'validate_only_comp_data': True, \n",
        "    \n",
        "    'boosting_type': 'gbdt',  # gbdt, rf, dart\n",
        "    'drop_rate': 0.1, \n",
        "    'max_drop': 50, \n",
        "\n",
        "    'n_estimators': 9999, \n",
        "    'early_stopping_rounds': 200, \n",
        "\n",
        "    'colsample_bytree': 1.0, \n",
        "    'subsample': 0.8,  \n",
        "    \n",
        "    'max_depth': 8, \n",
        "    'max_leaves': 64,   \n",
        "    'learning_rate': 0.05, \n",
        "\n",
        "    'reg_alpha': 0.0,  # Default: 0\n",
        "    'reg_lambda': 1.0,  # Default: 0\n",
        "    \n",
        "    'max_bin': 256,  # Default: 256\n",
        "    'min_data_in_leaf': 1, # Default: 1\n",
        "    'gamma': 0,   # default: 0\n",
        "\n",
        "    # if 'auto'; will be overridden as sum(negative instances) / sum(positive instances). \n",
        "    # Else; provided value will be used.\n",
        "    'scale_pos_weight': 'auto',  # 'auto' / 10 / 25 etc.\n",
        "\n",
        "    'verbosity': -1,\n",
        "\n",
        "    'objective': 'rmse', \n",
        "    'eval_metric': 'rmse',  \n",
        "    \n",
        "    'use_gpu_if_available': True, \n",
        "    'predictor': 'gpu_predictor',\n",
        "    'use_wandb': False, # Defaults to true if choice==3.\n",
        "    'n_jobs': -1, \n",
        "    'data_dir': '/content/data/', \n",
        "    'drive_project_dir': '/content/drive/MyDrive/Playground Series/S03_E08', \n",
        "    'project_name': 'playground_s03_e08', \n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# if config['use_gpu_if_available']:\n",
        "#   if torch.cuda.is_available():\n",
        "#     config['gpu_id'] = 0\n",
        "#     config['tree_method'] = 'gpu_hist'\n",
        "#     config['predictor'] = 'gpu_predictor'\n",
        "#     print(\"GPU available... XGBoost will use GPU...\")\n",
        "#   else:\n",
        "#     print(\"NOT USING GPU!!!!!! Parameter 'use_gpu_if_available' is set to TRUE; But NO GPU IS VISIBLE!!!!!\")\n",
        "#     if config['tree_method'] == 'gpu_hist': config['tree_method'] = 'hist'\n",
        "#     if config['predictor'] == 'gpu_predictor': config['predictor'] = 'cpu_predictor'\n",
        "# else:\n",
        "#   if config['tree_method'] == 'gpu_hist': config['tree_method'] = 'hist'\n",
        "#   if config['predictor'] == 'gpu_predictor': config['predictor'] = 'cpu_predictor' \n",
        "#   print(\"NOT USING GPU!!!!!! Parameter 'use_gpu_if_available' is set to False!!!!!!!\")    \n",
        "\n",
        "\n",
        "if config['choice'] == 3: config['use_wandb'] = True\n",
        "\n",
        "if config['use_wandb']:\n",
        "  os.environ['WANDB_MODE'] = 'online'\n",
        "  try: \n",
        "    wandb.login(key='d60ad29783a045de090c17001912975dc8f9f2e2') \n",
        "  except:\n",
        "    wandb.login()\n",
        "else:\n",
        "  os.environ['WANDB_MODE'] = 'offline'\n",
        "\n",
        "set_seeds(config)\n",
        "\n",
        "if config['choice'] == 1:\n",
        "  train_k_folds()\n",
        "elif config['choice'] == 2:\n",
        "  test_model(config)\n",
        "elif config['choice'] == 3:\n",
        "  sweep_configs = {\n",
        "      \"method\": \"bayes\",\n",
        "      \"metric\": {\n",
        "          \"name\": \"Fold average val rmse\",\n",
        "          \"goal\": \"minimize\",  # this wqkappa is output of sklearn function after predictions. SO can maximize.\n",
        "      },\n",
        "      \"parameters\": {      \n",
        "          \"colsample_bytree\": {\n",
        "              \"values\": [0.6, 0.8, 1.0]\n",
        "          },\n",
        "          \"subsample\": {\n",
        "              \"values\": [0.6, 0.8, 1.0]\n",
        "          },\n",
        "          \"max_depth\": {\n",
        "              \"values\": [5, 8, 12, 24]\n",
        "          },\n",
        "          'max_leaves': {\n",
        "              'values': [32, 64, 128, 256],\n",
        "          },\n",
        "          # \"reg_alpha\": {\n",
        "          #     \"values\": [0, 1.0, 2.0, 5.0]\n",
        "          # },\n",
        "          \"reg_lambda\": {\n",
        "              \"values\": [1.0, 2.0, 5.0]\n",
        "          },\n",
        "\n",
        "          # \"learning_rate\": {\n",
        "          #     \"values\": [0.01, 0.05, 0.1, 0.3, 0.5]\n",
        "          # }\n",
        "          # \"random_state\": {\n",
        "          #     \"values\": [0.01, 0.05, 0.1, 0.3, 0.5]\n",
        "          # },\n",
        "          # \"max_bin\": {\n",
        "          #     \"values\": [64, 128, 256, 512, 1024],\n",
        "          # },\n",
        "          # \"min_data_in_leaf\": {\n",
        "          #     \"values\": [1, 32, 64, 128, 256],\n",
        "          # },\n",
        "          \n",
        "        }\n",
        "  }\n",
        "  print(\"Running sweep>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
        "  sweep_id = wandb.sweep(sweep=sweep_configs, project=config['project_name']+'_sweep')\n",
        "  wandb.agent(sweep_id=sweep_id, function=train_k_folds, count=50)\n",
        "elif config['choice'] == 4:\n",
        "  model_analysis()\n",
        "else:\n",
        "  raise ValueError(f\"Incorrect value for 'choice'={config['choice']} in config\")\n",
        "\n"
      ],
      "metadata": {
        "id": "MoQ2_0Dbzccr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2638512c-0542-45d3-d745-3bd191b7c4ca"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read shape: train_df.shape:  (193573, 11)\n",
            "Read shape: test_df.shape:  (129050, 10)\n",
            "Read shape: sub_df.shape:  (129050, 2)\n",
            "train_df.shape:  (193573, 12)\n",
            "test_df.shape:  (129050, 10)\n",
            "sub_df.shape:  (129050, 2)\n",
            "orig_data_df.shape:  (26967, 11)\n",
            "model_training_config:  {'aggr_type': 'rmse_weighted', 'bool_cols': [], 'boosting_type': 'gbdt', 'choice': 1, 'colsample_bytree': 1.0, 'data_dir': '/content/data/', 'drive_data_dir': '/content/drive/MyDrive/Playground Series/S03_E08/data/', 'drive_feature_dir': '/content/drive/MyDrive/Playground Series/S03_E08/feature_store', 'drive_model_dir': '/content/drive/MyDrive/Playground Series/S03_E08/model_store/lgb_model_1', 'drive_project_dir': '/content/drive/MyDrive/Playground Series/S03_E08', 'drop_rate': 0.1, 'early_stopping_rounds': 200, 'enable_categorical': False, 'eval_metric': 'rmse', 'feature_cols': ['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z', 'volume', 'density', 'table_percentage', 'depth_percentage', 'symmetry', 'surface_area', 'depth_to_table_ratio'], 'feature_version': 'v1', 'fold_split_type': 'kfold', 'folds_to_train': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'gamma': 0, 'handle_outliers': False, 'id_cols': ['id', 'original_data'], 'include_orig_data': True, 'learning_rate': 0.05, 'local_data_dir': '/content/data', 'local_feature_dir': '/content/feature_store', 'local_model_dir': '/content/model_store/lgb_model_1', 'max_bin': 256, 'max_depth': 8, 'max_drop': 50, 'max_leaves': 64, 'min_data_in_leaf': 1, 'model_name': 'lgb_model_1', 'n_estimators': 9999, 'n_jobs': -1, 'num_folds': 10, 'objective': 'rmse', 'outlier_handling_method': 'winsorize', 'predict_log': False, 'predictor': 'gpu_predictor', 'project_name': 'playground_s03_e08', 'random_state': 21, 'reg_alpha': 0.0, 'reg_lambda': 1.0, 'scale_data': False, 'scale_pos_weight': 'auto', 'scaler_type': 'standard', 'subsample': 0.8, 'target_cols': ['price'], 'use_gpu_if_available': True, 'use_wandb': False, 'validate_only_comp_data': True, 'verbosity': -1}\n",
            "After appending orig data to train data: \n",
            "train_df.shape:  (220540, 12)\n",
            "After feature encoding: train_df.shape:  (220540, 13)\n",
            "After feature encoding: test_df.shape:  (129050, 10)\n",
            "After preprocessing 1: train_df.shape:  (220540, 13)\n",
            "After preprocessing 2: test_df.shape:  (129050, 10)\n",
            "Before feature extraction: train_df.shape:  (220540, 13)\n",
            "Before feature extraction: test_df.shape:  (129050, 10)\n",
            "After feature extraction: train_df.shape:  (220540, 20)\n",
            "After feature extraction: test_df.shape:  (129050, 17)\n",
            "# feature_cols:  16\n",
            "Training fold:  0\n",
            "len(train_idx): 201182 \t len(val_idx): 19358\n",
            "Loading model...\n",
            "MSE: Train: 291627.34964766185 \t Val: 304757.26419871993\n",
            "RMSE: Train: 540.0253231540738 \t Val: 552.0482444485444\n",
            "Training fold:  1\n",
            "len(train_idx): 201182 \t len(val_idx): 19358\n",
            "Loading model...\n",
            "MSE: Train: 307074.1973096308 \t Val: 324367.2762437571\n",
            "RMSE: Train: 554.1427589616513 \t Val: 569.5325067489625\n",
            "Training fold:  2\n",
            "len(train_idx): 201182 \t len(val_idx): 19358\n",
            "Loading model...\n",
            "MSE: Train: 312931.1063099584 \t Val: 336348.5026673704\n",
            "RMSE: Train: 559.4024546871049 \t Val: 579.9556040485948\n",
            "Training fold:  3\n",
            "len(train_idx): 201183 \t len(val_idx): 19357\n",
            "Loading model...\n",
            "MSE: Train: 305741.6016892449 \t Val: 359245.2471605537\n",
            "RMSE: Train: 552.9390578438504 \t Val: 599.3707092948017\n",
            "Training fold:  4\n",
            "len(train_idx): 201183 \t len(val_idx): 19357\n",
            "Loading model...\n",
            "MSE: Train: 298991.9367546523 \t Val: 368886.9277076668\n",
            "RMSE: Train: 546.8015515291195 \t Val: 607.3606241004325\n",
            "Training fold:  5\n",
            "len(train_idx): 201183 \t len(val_idx): 19357\n",
            "Loading model...\n",
            "MSE: Train: 296030.14195338957 \t Val: 333394.5404809117\n",
            "RMSE: Train: 544.086520650337 \t Val: 577.4032737012423\n",
            "Training fold:  6\n",
            "len(train_idx): 201183 \t len(val_idx): 19357\n",
            "Loading model...\n",
            "MSE: Train: 289997.75610793947 \t Val: 320931.3750983581\n",
            "RMSE: Train: 538.5143973079452 \t Val: 566.5080538689261\n",
            "Training fold:  7\n",
            "len(train_idx): 201183 \t len(val_idx): 19357\n",
            "Loading model...\n",
            "MSE: Train: 302629.23229537037 \t Val: 324272.90682087996\n",
            "RMSE: Train: 550.1174713598637 \t Val: 569.4496525777147\n",
            "Training fold:  8\n",
            "len(train_idx): 201183 \t len(val_idx): 19357\n",
            "Loading model...\n",
            "MSE: Train: 290019.0826252404 \t Val: 323193.8768617637\n",
            "RMSE: Train: 538.5341981947297 \t Val: 568.5014308352827\n",
            "Training fold:  9\n",
            "len(train_idx): 201183 \t len(val_idx): 19357\n",
            "Loading model...\n",
            "MSE: Train: 311117.7277333487 \t Val: 310428.1504980176\n",
            "RMSE: Train: 557.7792822733278 \t Val: 557.1607941142463\n",
            "Fold average stats.: \n",
            "RMSE: Train: 548.2343015962003 \t Val: 574.7290893738748\n",
            "MSE Loss: Train: 300616.0132426437 \t Val: 330582.60677379987\n",
            "per_fold_test_preds.shape:  (129050, 10)\n",
            "test_preds_aggr[:10]:  [ 1310.30938179  3038.05918045  3092.47316479  1340.11725089\n",
            "  6535.08619874   953.26743662 12351.62550964  4275.76561775\n",
            " 15588.7828132   3064.65347382]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "tOOF3Qlcfjpz"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raise"
      ],
      "metadata": {
        "id": "flCOWKEBzcCe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "fa8374e2-2a20-4a28-b2e6-4245caa82c36"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-9c9a2cba73bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "\n",
        "Original dataset: \n",
        "https://www.kaggle.com/datasets/colearninglounge/gemstone-price-prediction\n",
        "\n",
        "\n",
        "Problem Statement: \n",
        "You are hired by a company Gem Stones co ltd, which is a cubic zirconia manufacturer. \n",
        "You are provided with the dataset containing the prices and other attributes of almost 27,000 cubic zirconia \n",
        "(which is an inexpensive diamond alternative with many of the same qualities as a diamond). \n",
        "The company is earning different profits on different prize slots. You have to help the company in predicting \n",
        "the price for the stone on the basis of the details given in the dataset so it can distinguish between higher \n",
        "profitable stones and lower profitable stones so as to have a better profit share. \n",
        "Also, provide them with the best 5 attributes that are most important.\n",
        "\n",
        "\n",
        "Attributes:\n",
        "Carat:   Carat weight of the cubic zirconia.\n",
        "Cut:     Describe the cut quality of the cubic zirconia.\n",
        "         Quality is increasing order Fair, Good, Very Good, Premium, Ideal.\n",
        "Color:   Colour of the cubic zirconia.With D being the best and J the worst.\n",
        "Clarity: Cubic zirconia Clarity refers to the absence of the Inclusions and Blemishes.\n",
        "         (In order from Best to Worst, FL = flawless, I3= level 3 inclusions) \n",
        "         FL, IF, VVS1, VVS2, VS1, VS2, SI1, SI2, I1, I2, I3\n",
        "Depth:   The Height of a cubic zirconia, measured from the Culet to the table, divided by its average Girdle Diameter.\n",
        "Table:   The Width of the cubic zirconia's Table expressed as a Percentage of its Average Diameter.\n",
        "Price:   The Price of the cubic zirconia.\n",
        "X:       Length of the cubic zirconia in mm.\n",
        "Y:       Width of the cubic zirconia in mm.\n",
        "Z:       Height of the cubic zirconia in mm.\n",
        "\n",
        "\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "RZSOzMNEzb-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# \n",
        "# \n",
        "# Carat: ordinal, continuous. Do not encode for now.\n",
        "# Cut: ordinal, discrete. Encode with dict: {Fair: 1, Good: 2, Very Good: 3, Premium: 4, Ideal: 5}.\n",
        "# Color: ordinal, discrete. Encode with dict: {J: 1, I: 2, H: 3, G: 4, F: 5, E: 6, D: 7}\n",
        "# Clarity: ordinal, discrete. \n",
        "#           Encode with dict: {FL: 1, IF: 2, VVS1: 3, VVS2: 4, VS1: 5, VS2: 6, SI1: 7, SI2: 8, I1: 9, I2: 10, I3: 11}\n",
        "# Depth: ordinal, continuous. Do not encode for now.\n",
        "# Table: ordinal, continuous. Do not encode for now.\n",
        "# Price: ordinal, continuous. Do not encode for now.\n",
        "# X: ordinal, continuous. Do not encode for now.\n",
        "# Y: ordinal, continuous. Do not encode for now.\n",
        "# Z: ordinal, continuous. Do not encode for now.\n",
        "# \n",
        "# \n"
      ],
      "metadata": {
        "id": "PfpCXKBHd-vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UuEelh0azb5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "5MxpyMgrUrMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "base_dir_path = '/content/data'\n",
        "\n",
        "train_df = pd.read_csv(base_dir_path + '/train.csv')\n",
        "test_df = pd.read_csv(base_dir_path + '/test.csv')\n",
        "orig_data_df = pd.read_csv(base_dir_path + '/orig_data.csv')\n",
        "descriptor_df = pd.read_excel(base_dir_path + '/descriptor_dict.xlsx')\n"
      ],
      "metadata": {
        "id": "CygQ1DqdUrJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train_df.shape: \", train_df.shape)\n",
        "print(\"test_df.shape: \", test_df.shape)\n",
        "print(\"orig_data_df.shape: \", orig_data_df.shape)\n",
        "print(\"descriptor_df.shape: \", descriptor_df.shape)\n"
      ],
      "metadata": {
        "id": "H8GAYV72UrGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.columns"
      ],
      "metadata": {
        "id": "Mt7ub7NJUrCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "descriptor_df"
      ],
      "metadata": {
        "id": "MO7EbpOAKiE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orig_data_df.columns"
      ],
      "metadata": {
        "id": "PKI8jWn0X6EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# np.sum(np.isnan(train_df.color.values))\n",
        "\n"
      ],
      "metadata": {
        "id": "crMjQ_tEX6BQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# np.sum(np.isnan(orig_data_df.depth.values))\n"
      ],
      "metadata": {
        "id": "mgREjmXXX5-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0DUFkrFnX57V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}